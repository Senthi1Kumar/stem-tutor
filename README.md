# STEM Tutor

[![Python 3.12](https://img.shields.io/badge/Python-3.12-blue.svg?logo=python&logoColor=white)](https://www.python.org/)
[![CUDA 12.6](https://img.shields.io/badge/CUDA-12.x-lightgreen.svg?logo=NVIDIA)](https://developer.nvidia.com/cuda-toolkit)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

An AI-powered STEM tutoring system with voice interaction capabilities, optimized for NVIDIA GPUs.

https://github.com/user-attachments/assets/2b5e9b2f-694a-4893-9772-8b4d51854e5e

## Features

**Voice Interaction**  
- Speech Recognition: Fast audio processing using _Whisper-tiny_ via [CTranslate2](https://github.com/OpenNMT/CTranslate2/) inference engine.
- Speech Synthesis: Natural-sounding voice output using [Coqui TTS](https://github.com/idiap/coqui-ai-TTS)'s VITS model.
- Multi-modal input: Supports both voice recordings and text input.

**AI Tutor Core**  
- Model Architecture: Built on a  Fine-tuned [Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) model.
- Training data: Leveraged a specialized [Science conversations dataset](https://huggingface.co/datasets/jeffmeloy/sonnet3.5_science_conversations) generated by Claude's Sonnet-3.5.
- Optimizations:
    - 4-bit quantized inference
    - LoRA adapter enabling targeted fine-tuning
    - Extended sequence length support up to 1024 tokens.

**Monitoring**
- Real-time metrics: GPU/CPU usage, VRAM consumption, and latency through Prometheus.
- Access metrics at http://localhost:9090/


## Hardware Requirements

### Minimum
- NVIDIA GPU with 4GB VRAM (Ampere)
- CUDA 12.x driver
- 8GB System RAM
- Windows WSL2 or Ubuntu 24.04

## Installation

Clone the repository, set up a virtual environment, and install the dependencies:

```bash
git clone https://github.com/Senthi1Kumar/stem-tutor.git
cd stem-tutor

# Create virtual environment either using `venv` or `uv`, to install uv - https://docs.astral.sh/uv/getting-started/installation/#standalone-installer:

# Using venv
python -m venv .venv 

# Using uv (optional)
uv venv .venv

# Activate it:
source .venv/bin/activate # Windows: .venv\Scripts/activate

# Install the requirements:
# For venv
pip install -r requirements.txt

# For uv
uv pip install -r requirements.txt
```
> **Note**: Please install `unsloth` according to your platform and library specs at [here](https://docs.unsloth.ai/get-started/installing-+-updating)

Start the application
```bash
python3 gradio_app/app.py
```

Access interfaces:
- App UI: http://localhost:7860/
- Metrics: http://localhost:9090/


## Component Details

### STT 

- Current model: `whisper-tiny` (optimized with CTranslate2)
- Alternate models: If you want to use other Whisper variants, that should be compatible with CTranslate2 engine, please check [here](https://opennmt.net/CTranslate2/guides/transformers.html#whisper). For eg., if you want to use _whisper-medium_ model, run the command below, where it converts the pre-trained model from HuggingFace to CTranslate2 format:
```bash
ct2-transformers-converter --model openai/whisper-medium --output_dir whisper-medium-ct2
```
after that move `whisper-medium-ct2` under `models` folder, and adjust the path in `conf/stt/whisper.yaml`

### LLM

- Base Model: `meta-llama/Llama-3.2-3B-Instruct`
- Fine-Tuning:
    - Method: LoRA
    - Dataset: 8.8k science Q&A pairs from Sonnet-3.5
    - Libraries:
        - [Unsloth](https://github.com/unslothai/unsloth) for 4-bit training and inference.
        - [TRL](https://huggingface.co/docs/trl/en/index) for model alignment.
        - [W&B](https://wandb.ai) for tracking the training process
    
- Optimizations:
    - Inference with 4-bit quantization

## TTS

- Model: `tts_models/en/vctk/vits`
- Features:
    - Real-time audio synthesis
    - Over 109 English speaker voices
- Configuration: You can adjust or change the models in `conf/tts/coqui.yaml`

## Configurations

All configurable parameters are defined within the `conf/` directory. You can modify:
- STT : `conf/stt/whisper.yaml`
- LLM : `conf/llm/llama.yaml`
- TTS : `conf/tts/coqui.yaml`

Adjust these yaml files to change the model variants, voice profiles, sequence length as needed.

## Next Steps
- To-do: evaluate the fine-tuned model 
- Extend TTS component to support Indic languages and more natural expressions

